## TaPas论文阅读笔记

2021.9.12

### TaPas
论文：《TaPas:Weakly Supervised Table Parsing via Pre-training》

ACL2020

背景：回答和表格相关的自然语言问题通常被看作语义分析任务，这样产生的逻辑表达式只是作为中介。

Tapas是一种不需要生成逻辑表达式，直接通过表格回答问题的方法。TaPas从弱监督中得到训练，并通过选择表格单元格和选择性地应用相应的聚合运算符来预测结果。TaPas在Bert的结构上进行扩展，使用表格作为输入，利用从Wikipedia爬取的表格和文本片段进行端到端的预训练。


**论文指出之前研究的问题：**

1.通常将和表格相关的问答任务看作是语义分析问题。语义分析依赖于有监督的训练，数据的标注的代价很昂贵。因此弱监督学习方法诞生，但对于语义分析任务来说，弱监督学习很难学到丰富的逻辑表达式。

2.语义分析生成的逻辑表达式，大多是时候也只是获得答案的中介，不但如此，反而造成了许多问题。

**论文创新点：**

1. 提出TaPas，一个弱监督的表格问答模型，跳过了生成逻辑表达式的步骤。通过选择一种不需要生成逻辑表达式，直接通过表格回答问题的方法。扩展了BERT的结构，不需要额外的编码来捕获表格结构，最后仅接两层分类器来对表格单元和聚合运算符进行问题答案的预测。

2. 提出Tapas的预训练方法。在Bert的遮罩语言模型上做了扩展。在文本段和表格上遮罩部分token，通过文本和表格上下文来预测被遮罩信息。

3. 提出端对端的弱监督训练方法。从单元格中挑选构成子集，然后进行特定的聚合操作。

**优点：**

结构简单，在三个不同的语义分析数据集上，效果超过了其他的语义分析和问答模型。

---

#### 模型

TaPas的结构（如图3）基于BERT编码器，并添加了额外的位置embedding用于编码表的结构。该模型首先将表格平铺成单词序列，并将单词分割成wordpiece（token），并将其连接到问题token之后。此外模型还添加了两个分类层，用于选择单元格和对单元格进行操作的聚合操作符。

在被送入模型之前，token embedding需要与位置embedding进行结合，模型试用了一下位置embedding：

位置ID：token在序列中的索引（与BERT相同）
片段ID：有两个值：0表示描述，1表示表头和单元值
列、行ID：列、行的索引值。0表示自然语言描述
序数ID：如果一列的值可以被转换为浮点数或日期，就将它们进行排序，基于它们的叙述给定对应的embedding（0表示无法比较，1表示最小，以此类推）这可以帮助模型处理涉及最高级的问题，因为单词片段可能无法在信息上代表数字
历史答案：在多轮对话的设置中，当前问题可能指示了之前问题或者其答案，于是添加了一个特殊标记：如果该token为前一问题的答案则为1，否则为0。

![tapas的总体图](Paperphoto/tapas_overview.png)

分类层选取表中单元值的一个子集。由于还可能存在聚合操作，这些单元值可以是最终答案，也可以是用于计算最终答案的输入。每个单元值被建模为伯努利分布，首先计算每个token的logit值，随后计算单元内所有token的logit的平均值作为当前单元的logit值。然后选取所有概率大于0.5的单元格。

此外作者发现，在单一列中选取单元值可以起到一定作用。模型添加了一个分类变量来选取正确的列，通过计算一列中所有单元值的平均值embedding，经过一个线性层得到该列的logit值。此外还添加了一个单独的列，表示不选取任何单元格。

聚合操作预测

语义解析任务通常需要对表格进行推理，如求和、计算平均值等。为了在不生成逻辑形式的情况下处理这些情况，TaPas需要对预测的单元值给定一个聚合操作符。操作符由一个线性层选择，在第一个token的最后一层应用softmax得到选取每一中操作符的概率。

---

#### 预训练过程####


**训练数据：**

从维基百科中提取了620万条表格和文本数据，包括330万个infobox和290万个WikiTable。并且只考虑少于500个单元格的表。

**MLM预训练任务：**

类似于TaBert，TaPas也采用了MLM（masked language model）作为预训练目标。同时还尝试添加了另一个训练目标：判断表格是否符合文本描述，但是发现对于我们的任务并没有提升。

为了使预训更有效率，我们将我们的片段序列长度限制在一定的预算内(例如，我们在最后的实验中使用128)。这是组合长度的标记文本和表格单元格必须适合这个预算。为了实现这一点，我们从相关文本中随机选择一个8到16个单词的片段。这张桌子很合适。首先，我们只添加每个列名和单元格的第一个单词。然后，我们不断添加单词的转向，直到我们达到单词计件预算。对于每个表，我们用这种方法生成10个不同的代码片段

我们遵循 BERT 提出的掩蔽过程。我们对文本使用整个单词屏蔽，我们发现将整个 celi 屏蔽(如果单元格的任何部分被屏蔽，则屏蔽单元格的所有单词部分)也应用到表中是有益的

#### 局限性

TaPas将单个表作为上下文进行处理，而且限制了最大序列长度。因此TaPas无法处理非常大的表和多个表的数据库。这就需要对多表进行过滤和压缩，只对相关的内容进行编码，这是后续需要进行的工作。

TaPas尽管可以处理组合的操作（如图5的问题2），但是仅限于对一个表单元格子集上的聚合，无法处理具有多个聚合操作的结构（比如：给出参与者平均分大于4的人数）。尽管存在这一限制，TaPas仍然在3个数据集上取得了不错的效果，而且并没有遇到类似的错误。这说明语义解析数据集在组合性上还是存在局限性

---

**总结：**

TaPas作为一个弱监督的语义解析模型，避免了生成逻辑表达式，并且有效地训练了大规模的“文本-表格”对的数据，成功预测了被遮蔽的词和单元格。

模型可以仅使用弱监督的方法，在语义解析数据集上进行微调。实验结果显示，与优秀的语义解析模型相比，TaPas具有更好或更有竞争力的效果。

TaBert和TaPas虽然在相关任务上取得了很大的进度，但是提供的还都是单表格context，对于一些复杂的任务，需要同时编码多个表格，未来的工作可以1）在multi-table context进行探索 2）提高用于预训练的表格及描述的质量 3）探索更多的无监督预训练目标函数 4）在更多的与表格相关的任务上，如表格描述生成（table-to-text）做探索。
